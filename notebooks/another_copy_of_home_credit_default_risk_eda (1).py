# -*- coding: utf-8 -*-
"""Another copy of Home Credit Default Risk EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BfnesP_V7aUxf77byfqaYTbG05y35NGO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!pip -q install duckdb
import duckdb
con = duckdb.connect()

from google.colab import drive
drive.mount('/content/drive')

"""# Important Glossary



*   Home Credit - The institution thats providing the loan based on customers who do not have much of a credit score detail in their system, and the institution for whom we are building the model

*   Credit Bureau -   
    1.   Third party
    2.   Loans that client get from other instituions get reported to Credit Bureau
    3.   Home Credit can extract data from Credit Bureau

# **Exploring application_train.csv**

*   Data from **Home Credit**
*   application_train.csv has the target labels
*   One row represents one loan
*   Has other static data of customers
*   SK_ID_CURR is the customer unique indentifier
"""

application_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/application_train.csv')

application_df.head()

application_df.TARGET.value_counts()

application_df.shape

# adding number of missing values per row

application_df["TOT_MISSING_COUNT"] = application_df.isna().sum(axis=1)

application_df

EPS = 1e-6



application_df_preprocess = con.execute(f"""
SELECT

    ----------------- as-is features ---------------

    SK_ID_CURR,
    TARGET,

    EXT_SOURCE_1,
    EXT_SOURCE_2,
    EXT_SOURCE_3,

    AMT_ANNUITY,
    AMT_CREDIT,
    AMT_GOODS_PRICE,
    AMT_INCOME_TOTAL,

    DAYS_BIRTH,
    DAYS_EMPLOYED,
    DAYS_REGISTRATION,
    DAYS_ID_PUBLISH,
    DAYS_LAST_PHONE_CHANGE,

    CODE_GENDER,

    CNT_CHILDREN,
    CNT_FAM_MEMBERS,

    REGION_RATING_CLIENT,
    REGION_RATING_CLIENT_W_CITY,
    REGION_POPULATION_RELATIVE,

    WEEKDAY_APPR_PROCESS_START,
    HOUR_APPR_PROCESS_START,

    NAME_INCOME_TYPE,
    NAME_EDUCATION_TYPE,
    NAME_HOUSING_TYPE,
    NAME_CONTRACT_TYPE,
    NAME_FAMILY_STATUS,



    REG_REGION_NOT_LIVE_REGION,
    REG_REGION_NOT_WORK_REGION,
    LIVE_REGION_NOT_WORK_REGION,
    REG_CITY_NOT_LIVE_CITY,
    REG_CITY_NOT_WORK_CITY,
    LIVE_CITY_NOT_WORK_CITY,

    OBS_30_CNT_SOCIAL_CIRCLE,
    OBS_60_CNT_SOCIAL_CIRCLE,
    DEF_30_CNT_SOCIAL_CIRCLE,
    DEF_60_CNT_SOCIAL_CIRCLE,

    AMT_REQ_CREDIT_BUREAU_HOUR,
    AMT_REQ_CREDIT_BUREAU_DAY,
    AMT_REQ_CREDIT_BUREAU_WEEK,
    AMT_REQ_CREDIT_BUREAU_MON,
    AMT_REQ_CREDIT_BUREAU_QRT,
    AMT_REQ_CREDIT_BUREAU_YEAR,

    FLAG_MOBIL,
    FLAG_EMP_PHONE,
    FLAG_WORK_PHONE,
    FLAG_CONT_MOBILE,
    FLAG_PHONE,
    FLAG_EMAIL,

    FLAG_OWN_CAR,
    FLAG_OWN_REALTY,

    FLAG_DOCUMENT_2,
    FLAG_DOCUMENT_3,
    FLAG_DOCUMENT_4,
    FLAG_DOCUMENT_5,
    FLAG_DOCUMENT_6,
    FLAG_DOCUMENT_7,
    FLAG_DOCUMENT_8,
    FLAG_DOCUMENT_9,
    FLAG_DOCUMENT_10,
    FLAG_DOCUMENT_11,
    FLAG_DOCUMENT_12,
    FLAG_DOCUMENT_13,
    FLAG_DOCUMENT_14,
    FLAG_DOCUMENT_15,
    FLAG_DOCUMENT_16,
    FLAG_DOCUMENT_17,
    FLAG_DOCUMENT_18,
    FLAG_DOCUMENT_19,
    FLAG_DOCUMENT_20,
    FLAG_DOCUMENT_21,

    TOT_MISSING_COUNT,

    ----------------- modified time features ---------------

    -DAYS_BIRTH/(365 + {EPS}) AS AGE_YEARS,
    CASE WHEN DAYS_EMPLOYED = 365243 THEN 1 ELSE 0 END AS DAYS_EMPLOYED_ANOMALY,
    -CASE WHEN DAYS_EMPLOYED = 365243 THEN NULL ELSE DAYS_EMPLOYED END / (365 + {EPS}) AS EMPLOYED_YEARS,
    -CASE WHEN DAYS_EMPLOYED = 365243 THEN NULL ELSE DAYS_EMPLOYED END / (-DAYS_BIRTH + {EPS}) AS EMPLOYMENT_YEARS_TO_AGE,

    ----------------- ratio features ---------------

    AMT_ANNUITY/(AMT_CREDIT + {EPS}) AS PAYMENT_RATE,
    AMT_CREDIT/(AMT_INCOME_TOTAL + {EPS}) AS CREDIT_TO_INCOME,
    AMT_ANNUITY/(AMT_INCOME_TOTAL + {EPS}) AS ANNUITY_TO_INCOME,
    AMT_GOODS_PRICE/(AMT_CREDIT + {EPS}) AS GOODS_TO_CREDIT,
    AMT_INCOME_TOTAL/(CNT_FAM_MEMBERS + {EPS}) AS INCOME_PER_PERSON,
    CNT_CHILDREN/(CNT_FAM_MEMBERS + {EPS}) AS CHILDREN_RATIO,

    ----------------- missing-count features ---------------

    CASE WHEN EXT_SOURCE_1 IS NULL THEN 1 ELSE 0 END AS EXT_SOURCE_1_IS_MISSING,
    CASE WHEN EXT_SOURCE_2 IS NULL THEN 1 ELSE 0 END AS EXT_SOURCE_2_IS_MISSING,
    CASE WHEN EXT_SOURCE_3 IS NULL THEN 1 ELSE 0 END AS EXT_SOURCE_3_IS_MISSING,
    CASE WHEN OWN_CAR_AGE IS NULL THEN 1 ELSE 0 END AS  OWN_CAR_AGE_IS_MISSING,

    ----------------- count features ---------------

    COALESCE(FLAG_DOCUMENT_2,0) + COALESCE(FLAG_DOCUMENT_3,0) + COALESCE(FLAG_DOCUMENT_4,0) + COALESCE(FLAG_DOCUMENT_5,0) +
    COALESCE(FLAG_DOCUMENT_6,0) + COALESCE(FLAG_DOCUMENT_7,0) + COALESCE(FLAG_DOCUMENT_8,0) + COALESCE(FLAG_DOCUMENT_9,0) +
    COALESCE(FLAG_DOCUMENT_10,0) + COALESCE(FLAG_DOCUMENT_11,0) + COALESCE(FLAG_DOCUMENT_12,0) + COALESCE(FLAG_DOCUMENT_13,0) +
    COALESCE(FLAG_DOCUMENT_14,0) + COALESCE(FLAG_DOCUMENT_15,0) + COALESCE(FLAG_DOCUMENT_16,0) + COALESCE(FLAG_DOCUMENT_17,0) +
    COALESCE(FLAG_DOCUMENT_18,0) + COALESCE(FLAG_DOCUMENT_19,0) + COALESCE(FLAG_DOCUMENT_20,0) + COALESCE(FLAG_DOCUMENT_21,0)  AS DOC_COUNT,

    COALESCE(FLAG_MOBIL,0) + COALESCE(FLAG_EMP_PHONE,0) + COALESCE(FLAG_WORK_PHONE,0) +
    COALESCE(FLAG_CONT_MOBILE,0) + COALESCE(FLAG_PHONE,0) + COALESCE(FLAG_EMAIL,0) AS CONTACT_COUNT,

    COALESCE(REG_REGION_NOT_LIVE_REGION,0) + COALESCE(REG_REGION_NOT_WORK_REGION,0) + COALESCE(LIVE_REGION_NOT_WORK_REGION,0) +
    COALESCE(REG_CITY_NOT_LIVE_CITY,0) + COALESCE(REG_CITY_NOT_WORK_CITY,0) + COALESCE(LIVE_CITY_NOT_WORK_CITY,0) AS ADDR_MISMATCH_SUM

FROM
      application_df
""").df()

application_df_preprocess







"""# Bureau DF"""

bureau_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/bureau.csv')

bureau_df.head()

bureau_df_preprocess = con.execute(f"""
SELECT
      SK_ID_CURR,
      SUM(CASE WHEN CREDIT_ACTIVE = 'Active' THEN 1 ELSE 0 END) AS BUREAU_N_ACTIVE,
      SUM(CASE WHEN CREDIT_ACTIVE = 'Closed' THEN 1 ELSE 0 END) AS BUREAU_N_CLOSED,
      MAX(DAYS_CREDIT) AS BUREAU_LAST_CREDIT_DAYS,
      MAX(DAYS_CREDIT_UPDATE) AS BUREAU_LAST_UPDATE_DAYS,
      SUM(AMT_CREDIT_SUM_DEBT) AS BUREAU_TOTAL_DEBT,
      SUM(AMT_CREDIT_SUM) AS BUREAU_TOTAL_CREDIT,
      SUM(AMT_CREDIT_SUM_LIMIT) AS BUREAU_LIMIT_SUM,
      AVG(AMT_CREDIT_SUM_DEBT/(AMT_CREDIT_SUM + {EPS})) AS BUREAU_UTIL_MEAN,
      SUM(AMT_CREDIT_SUM_OVERDUE) AS BUREAU_OVERDUE_SUM,
      MAX(AMT_CREDIT_MAX_OVERDUE) AS BUREAU_MAX_OVERDUE,
      SUM(CNT_CREDIT_PROLONG) AS BUREAU_PROLONG_SUM
FROM
      bureau_df
GROUP BY
      SK_ID_CURR
""").df()

bureau_balance_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/bureau_balance.csv')

bureau_balance_df_preprocess = con.execute("""
WITH table_1 AS (
    SELECT
          *,
          CASE WHEN (STATUS = 'C' OR STATUS = 'X' OR STATUS = '0') THEN 0 ELSE CAST(STATUS AS INT) END AS STATUS_NUM
    FROM
          bureau_balance_df
),

table_2 AS (
    SELECT
        *,
        CASE WHEN STATUS_NUM > 0 THEN 1 ELSE 0 END AS IS_DELINQ
    FROM
        table_1

),

table_3 AS (
    SELECT
        SK_ID_BUREAU,
        AVG(IS_DELINQ) AS BB_DELINQ_SHARE,
        MAX(STATUS_NUM) AS BB_WORST_STATUS_NUM
    FROM
        table_2
    GROUP BY
        SK_ID_BUREAU

),

table_4 AS (
    SELECT
          SK_ID_BUREAU,
          -MAX(MONTHS_BALANCE) AS BB_MONTHS_LAST_DELINQ
    FROM
          table_2
    WHERE
          IS_DELINQ = 1
    GROUP BY
          SK_ID_BUREAU

)

SELECT
      A.*,
      B.BB_MONTHS_LAST_DELINQ
FROM
      table_3 A
LEFT JOIN
      table_4 B
ON
      A.SK_ID_BUREAU = B.SK_ID_BUREAU
""").df()

bureau_balance_join_df = con.execute("""
WITH table_1 AS (
    SELECT
          A.*,
          B.SK_ID_CURR
    FROM
          bureau_balance_df_preprocess A
    LEFT JOIN
          bureau_df B
    ON
          A.SK_ID_BUREAU = B.SK_ID_BUREAU
),

table_2 AS (
    SELECT
        SK_ID_CURR,
        AVG(BB_DELINQ_SHARE) AS BB_DELINQ_SHARE_MEAN,
        MAX(BB_WORST_STATUS_NUM) AS BB_WORST_STATUS_MAX,
        MIN(BB_MONTHS_LAST_DELINQ) AS BB_MONTHS_SINCE_LAST_DELINQ_MIN
    FROM
        table_1
    GROUP BY
        SK_ID_CURR
)

SELECT * FROM table_2
""").df()

bureau_df_preproces_2 = con.execute("""
SELECT
      A.*,
      B.BB_DELINQ_SHARE_MEAN,
      B.BB_WORST_STATUS_MAX,
      B.BB_MONTHS_SINCE_LAST_DELINQ_MIN
FROM
      bureau_df_preprocess A
LEFT JOIN
      bureau_balance_join_df B
ON
      A.SK_ID_CURR = B.SK_ID_CURR
""").df()

"""# Prev Application"""

prev_application_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/previous_application.csv')

prev_application_df_preprocess = con.execute("""
SELECT
      SK_ID_CURR,
      COUNT(DISTINCT SK_ID_PREV) AS PREV_TOTAL,

      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Approved' THEN 1 ELSE 0 END) AS PREV_APPROVED,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Refused' THEN 1 ELSE 0 END) AS PREV_REFUSED,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Canceled' THEN 1 ELSE 0 END) AS PREV_CANCELLED,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Unused offer' THEN 1 ELSE 0 END) AS PREV_UNUSED,

      AVG(AMT_APPLICATION/AMT_CREDIT) AS PREV_APP_CREDIT_RATIO_MEAN,
      AVG(AMT_DOWN_PAYMENT/AMT_CREDIT) AS PREV_DOWNPAYMENT_RATE_MEAN,

      AVG(CNT_PAYMENT) AS PREV_CNT_PAYMENT_MEAN,
      MAX(DAYS_DECISION) AS PREV_LAST_DECISION_DAYS,
      MIN(DAYS_DECISION) AS PREV_EARLIEST_DECISION_DAYS,

      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Approved' THEN 1 ELSE 0 END)/COUNT(DISTINCT SK_ID_PREV) AS PREV_APPROVAL_RATE,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Refused' THEN 1 ELSE 0 END)/COUNT(DISTINCT SK_ID_PREV) AS PREV_REFUSED_RATE

FROM
    prev_application_df
GROUP BY
    SK_ID_CURR
""").df()





"""#MODEL BUILDING 2"""

# A, B, C are your already-prepared DataFrames:
# A = application_df_preprocess
# B = bureau_df_preproces_2            # one row per SK_ID_CURR
# C = prev_application_df_preprocess   # one row per SK_ID_CURR

import pandas as pd

# keep exactly the columns the SQL selects
b_cols = [
    "SK_ID_CURR",
    "BUREAU_N_ACTIVE","BUREAU_N_CLOSED","BUREAU_LAST_CREDIT_DAYS","BUREAU_LAST_UPDATE_DAYS",
    "BUREAU_TOTAL_DEBT","BUREAU_TOTAL_CREDIT","BUREAU_LIMIT_SUM","BUREAU_UTIL_MEAN",
    "BUREAU_OVERDUE_SUM","BUREAU_MAX_OVERDUE","BUREAU_PROLONG_SUM",
    "BB_DELINQ_SHARE_MEAN","BB_WORST_STATUS_MAX","BB_MONTHS_SINCE_LAST_DELINQ_MIN"
]

c_cols = [
    "SK_ID_CURR",
    "PREV_TOTAL","PREV_APPROVED","PREV_REFUSED","PREV_CANCELLED","PREV_UNUSED",
    "PREV_APP_CREDIT_RATIO_MEAN","PREV_DOWNPAYMENT_RATE_MEAN","PREV_CNT_PAYMENT_MEAN",
    "PREV_LAST_DECISION_DAYS","PREV_EARLIEST_DECISION_DAYS",
    "PREV_APPROVAL_RATE","PREV_REFUSED_RATE"
]

feature_store_df = (
    application_df_preprocess
      .merge(bureau_df_preproces_2[b_cols], on="SK_ID_CURR", how="left", validate="one_to_one")
      .merge(prev_application_df_preprocess[c_cols], on="SK_ID_CURR", how="left", validate="one_to_one")
)

# feature_store_df = application_df_preprocess.copy()

categorical_columns = feature_store_df.select_dtypes(include=['object']).columns
categorical_columns

# one-hot encoding categorical columns

feature_store_df = pd.get_dummies(feature_store_df, columns=categorical_columns)
[col for col in feature_store_df.columns]

# impute missing values with median

feature_store_df = feature_store_df.fillna(feature_store_df.median(numeric_only=True))

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(feature_store_df, test_size= 0.2, stratify=feature_store_df.TARGET, random_state=42)

print(train_df.TARGET.value_counts())
print(test_df.TARGET.value_counts())

# Making balanced training set

target_1 = train_df[train_df.TARGET == 1]
target_0 = train_df[train_df.TARGET == 0].sample(n = target_1.shape[0])

print(target_0.shape[0], target_1.shape[0])

final_train_df = pd.concat([target_0, target_1], axis = 0)
print(final_train_df.TARGET.value_counts())

X_cols = [col for col in final_train_df.columns]
X_cols = [c for c in X_cols if c not in {"TARGET", "SK_ID_CURR"}]

y_col = 'TARGET'

from xgboost import XGBClassifier

model = XGBClassifier()

model.fit(final_train_df[X_cols], final_train_df[y_col])

y_pred = model.predict(test_df[X_cols])
y_test = test_df[y_col]

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix
)
import numpy as np



acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec  = recall_score(y_test, y_pred, zero_division=0)
f1   = f1_score(y_test, y_pred, zero_division=0)

print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}")

#  summary per class
print("\nClassification report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion matrix:\n", cm)

