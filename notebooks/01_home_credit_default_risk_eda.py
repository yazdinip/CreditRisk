# -*- coding: utf-8 -*-
"""Home Credit Default Risk EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DQQ8Nnd4DEp1aEMzT9BeOqTZSeJDFqoW
"""

!pip -q install duckdb
import duckdb
con = duckdb.connect()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

"""# Important Glossary



*   Home Credit - The institution thats providing the loan based on customers who do not have much of a credit score detail in their system, and the institution for whom we are building the model

*   Credit Bureau -   
    1.   Third party
    2.   Loans that client get from other instituions get reported to Credit Bureau
    3.   Home Credit can extract data from Credit Bureau

# **Exploring application_train.csv**

*   Data from **Home Credit**
*   application_train.csv has the target labels
*   One row represents one loan
*   Has other static data of customers
*   SK_ID_CURR is the customer unique indentifier
"""

application_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/application_train.csv')
con.register("application_df_vw", application_df)   # register pandas DF as a view

application_df.head()

application_df.TARGET.value_counts()

24825*100/282686

application_df.shape

[col for col in application_df.columns]

con.execute("""
SELECT
    COUNT(SK_ID_CURR),
    COUNT(DISTINCT SK_ID_CURR)
FROM
    application_df_vw
""").df()        # -> pandas DataFrame

application_df[application_df.EXT_SOURCE_3>= 0.7]['TARGET'].value_counts()

(application_df["DAYS_EMPLOYED"])

application_df

application_df["DAYS_EMPLOYED_ANOM"] = (application_df["DAYS_EMPLOYED"] == 365243).astype(int)
    application_df["DAYS_EMPLOYED_REPLACED"] = application_df["DAYS_EMPLOYED"].replace({365243: np.nan})

for col in ["EXT_SOURCE_1", "EXT_SOURCE_2", "EXT_SOURCE_3", "OWN_CAR_AGE"]:
    if col in application_df.columns:
        application_df[f"{col}_IS_MISSING"] = application_df[col].isna().astype(int)

application_df



"""# **Exploring bureau.csv**

* Data from **Credit Bureau**
* Application data from previous loans that client got from other institution that were reported to Credit Bureau
* One row per client's loan in Credit Bureau
"""

bureau_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/bureau.csv')
con.register("bureau_df_vw", bureau_df)

bureau_df.head()

bureau_df.shape

[col for col in bureau_df.columns]

con.execute("""
SELECT
    COUNT(SK_ID_CURR),
    COUNT(DISTINCT SK_ID_CURR)
FROM
    bureau_df_vw
""").df()

# Unique SK_ID_CURR common to application_df and buraeu_df

con.execute("""
SELECT
    COUNT(A.SK_ID_CURR)
FROM
    application_df_vw A
INNER JOIN
    (
      SELECT
          SK_ID_CURR
      FROM
          bureau_df_vw
      GROUP BY
          SK_ID_CURR
    ) B
ON
    A.SK_ID_CURR = B.SK_ID_CURR
""").df()

"""# **Exploring bureau_balance.csv**



*   Monthly balance of credits in Credit Bureau
*   Additional behavioural data
*   This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.




"""

bureau_balance_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/bureau_balance.csv')
con.register("bureau_balance_df_vw", bureau_balance_df)   # register pandas DF as a view

bureau_balance_df.head()

bureau_balance_df.shape

con.execute("""
SELECT
    COUNT(SK_ID_BUREAU),
    COUNT(DISTINCT SK_ID_BUREAU)
FROM
    bureau_balance_df_vw
""").df()

# Unique SK_ID_BUREAU common to bureau_df and bureau_balance_df

con.execute("""
SELECT
    COUNT(A.SK_ID_BUREAU)
FROM
    (
      SELECT
          SK_ID_BUREAU
      FROM
          bureau_df_vw
      GROUP BY
          SK_ID_BUREAU
    ) A
INNER JOIN
    (
      SELECT
          SK_ID_BUREAU
      FROM
          bureau_balance_df_vw
      GROUP BY
          SK_ID_BUREAU
    ) B
ON
    A.SK_ID_BUREAU = B.SK_ID_BUREAU
""").df()

"""# **Exploring POS_CASH_balance.csv:**

*   Monthly balance of client's previous loan in Home Credit
*   Additional Behavioural data

*   SK_ID_CURR is the unique identifier
*   This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.








"""

pos_cash_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/POS_CASH_balance.csv')
con.register("pos_cash_df_vw", pos_cash_df)   # register pandas DF as a view

pos_cash_df.head()

pos_cash_df.shape

con.execute("""
SELECT
    COUNT(SK_ID_CURR),
    COUNT(DISTINCT SK_ID_CURR)
FROM
    pos_cash_df_vw
""").df()

con.execute("""
SELECT
    *
FROM
    pos_cash_df_vw
WHERE
    SK_ID_CURR = 182943
""").df()

"""# **Basic Model to Get Started**"""

application_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/application_train.csv')
con.register("application_df_vw", application_df)   # register pandas DF as a view

application_df.TARGET.value_counts()

# checking null values per column

with pd.option_context('display.max_rows', None):
  display(application_df.isnull().sum())

# adding number of missing values per row

application_df["missing_count"] = application_df.isna().sum(axis=1)

with pd.option_context('display.max_columns', None):
  display(application_df.head())

# drop columns that has more then 40% missing values

drop_threshold_perc = 0.4
keep_threshold_perc = 1 - drop_threshold_perc
keep_threshold_count = keep_threshold_perc * application_df.shape[0]

print(application_df.shape)
application_df.dropna(thresh=keep_threshold_count, axis=1, inplace = True)
print(application_df.shape)

# columns with categorical data

application_df.select_dtypes(include=['object'])

# missing values of categorical columns

application_df.select_dtypes(include=['object']).isnull().sum()

# Dropping categorical columns that has any missing values, and also ORGANIZATION_TYPE because it has over 50 unique values to one-hot

application_df.drop(columns=['NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE'], inplace=True)
print(application_df.shape)

categorical_columns = application_df.select_dtypes(include=['object']).columns
categorical_columns

# one-hot encoding categorical columns

application_df = pd.get_dummies(application_df, columns=categorical_columns)
[col for col in application_df.columns]

with pd.option_context('display.max_rows', None):
  display(application_df.isnull().sum())

# impute missing values with median

application_df = application_df.fillna(application_df.median(numeric_only=True))

application_df

application_df.head()

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE


# train-test split

X = application_df.drop(columns = 'TARGET')
y = application_df['TARGET']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, stratify=y, random_state=42)

y_train.value_counts()

# SMOTE on the training set only (balances 0s and 1s by default)

smote = SMOTE(random_state=42, sampling_strategy=0.2)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(y_train.value_counts())

print(y_train_resampled.value_counts())

X_train_resampled

y_train_resampled

# concating the two dfs

train_resampled = pd.concat([X_train_resampled, y_train_resampled],axis=1)
train_resampled

[col for col in train_resampled.columns]

X_cols = [
 'CNT_CHILDREN',
 'AMT_INCOME_TOTAL',
 'AMT_CREDIT',
 'AMT_ANNUITY',
 'AMT_GOODS_PRICE',
 'REGION_POPULATION_RELATIVE',
 'DAYS_BIRTH',
 'DAYS_EMPLOYED',
 'DAYS_REGISTRATION',
 'DAYS_ID_PUBLISH',
 'FLAG_MOBIL',
 'FLAG_EMP_PHONE',
 'FLAG_WORK_PHONE',
 'FLAG_CONT_MOBILE',
 'FLAG_PHONE',
 'FLAG_EMAIL',
 'CNT_FAM_MEMBERS',
 'REGION_RATING_CLIENT',
 'REGION_RATING_CLIENT_W_CITY',
 'HOUR_APPR_PROCESS_START',
 'REG_REGION_NOT_LIVE_REGION',
 'REG_REGION_NOT_WORK_REGION',
 'LIVE_REGION_NOT_WORK_REGION',
 'REG_CITY_NOT_LIVE_CITY',
 'REG_CITY_NOT_WORK_CITY',
 'LIVE_CITY_NOT_WORK_CITY',
 'EXT_SOURCE_2',
 'EXT_SOURCE_3',
 'OBS_30_CNT_SOCIAL_CIRCLE',
 'DEF_30_CNT_SOCIAL_CIRCLE',
 'OBS_60_CNT_SOCIAL_CIRCLE',
 'DEF_60_CNT_SOCIAL_CIRCLE',
 'DAYS_LAST_PHONE_CHANGE',
 'FLAG_DOCUMENT_2',
 'FLAG_DOCUMENT_3',
 'FLAG_DOCUMENT_4',
 'FLAG_DOCUMENT_5',
 'FLAG_DOCUMENT_6',
 'FLAG_DOCUMENT_7',
 'FLAG_DOCUMENT_8',
 'FLAG_DOCUMENT_9',
 'FLAG_DOCUMENT_10',
 'FLAG_DOCUMENT_11',
 'FLAG_DOCUMENT_12',
 'FLAG_DOCUMENT_13',
 'FLAG_DOCUMENT_14',
 'FLAG_DOCUMENT_15',
 'FLAG_DOCUMENT_16',
 'FLAG_DOCUMENT_17',
 'FLAG_DOCUMENT_18',
 'FLAG_DOCUMENT_19',
 'FLAG_DOCUMENT_20',
 'FLAG_DOCUMENT_21',
 'AMT_REQ_CREDIT_BUREAU_HOUR',
 'AMT_REQ_CREDIT_BUREAU_DAY',
 'AMT_REQ_CREDIT_BUREAU_WEEK',
 'AMT_REQ_CREDIT_BUREAU_MON',
 'AMT_REQ_CREDIT_BUREAU_QRT',
 'AMT_REQ_CREDIT_BUREAU_YEAR',
 'missing_count',
 'NAME_CONTRACT_TYPE_Cash loans',
 'NAME_CONTRACT_TYPE_Revolving loans',
 'CODE_GENDER_F',
 'CODE_GENDER_M',
 'CODE_GENDER_XNA',
 'FLAG_OWN_CAR_N',
 'FLAG_OWN_CAR_Y',
 'FLAG_OWN_REALTY_N',
 'FLAG_OWN_REALTY_Y',
 'NAME_INCOME_TYPE_Businessman',
 'NAME_INCOME_TYPE_Commercial associate',
 'NAME_INCOME_TYPE_Maternity leave',
 'NAME_INCOME_TYPE_Pensioner',
 'NAME_INCOME_TYPE_State servant',
 'NAME_INCOME_TYPE_Student',
 'NAME_INCOME_TYPE_Unemployed',
 'NAME_INCOME_TYPE_Working',
 'NAME_EDUCATION_TYPE_Academic degree',
 'NAME_EDUCATION_TYPE_Higher education',
 'NAME_EDUCATION_TYPE_Incomplete higher',
 'NAME_EDUCATION_TYPE_Lower secondary',
 'NAME_EDUCATION_TYPE_Secondary / secondary special',
 'NAME_FAMILY_STATUS_Civil marriage',
 'NAME_FAMILY_STATUS_Married',
 'NAME_FAMILY_STATUS_Separated',
 'NAME_FAMILY_STATUS_Single / not married',
 'NAME_FAMILY_STATUS_Unknown',
 'NAME_FAMILY_STATUS_Widow',
 'NAME_HOUSING_TYPE_Co-op apartment',
 'NAME_HOUSING_TYPE_House / apartment',
 'NAME_HOUSING_TYPE_Municipal apartment',
 'NAME_HOUSING_TYPE_Office apartment',
 'NAME_HOUSING_TYPE_Rented apartment',
 'NAME_HOUSING_TYPE_With parents',
 'WEEKDAY_APPR_PROCESS_START_FRIDAY',
 'WEEKDAY_APPR_PROCESS_START_MONDAY',
 'WEEKDAY_APPR_PROCESS_START_SATURDAY',
 'WEEKDAY_APPR_PROCESS_START_SUNDAY',
 'WEEKDAY_APPR_PROCESS_START_THURSDAY',
 'WEEKDAY_APPR_PROCESS_START_TUESDAY',
 'WEEKDAY_APPR_PROCESS_START_WEDNESDAY',
 ]

y_col = 'TARGET'

# Making balanced training set

target_1 = train_resampled[train_resampled.TARGET == 1]
target_0 = train_resampled.sample(n = target_1.shape[0])

print(target_0.shape[0], target_1.shape[0])

final_train_df = pd.concat([target_0, target_1], axis = 0)
print(final_train_df.shape[0])

# Applying Standard Scaler

from sklearn.preprocessing import StandardScaler

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the scaler to the data and transform it
X_train_scaled = scaler.fit_transform(final_train_df[X_cols])

X_train_scaled

from xgboost import XGBClassifier

model = XGBClassifier()

model.fit(X_train_scaled, final_train_df.TARGET.values)

"""# TESTING"""

X_test_scaled = scaler.transform(X_test[X_cols])

y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)



y_test

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix
)
import numpy as np



acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec  = recall_score(y_test, y_pred, zero_division=0)
f1   = f1_score(y_test, y_pred, zero_division=0)

print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}")

#  summary per class
print("\nClassification report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion matrix:\n", cm)

