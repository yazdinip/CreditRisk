# -*- coding: utf-8 -*-
"""Another copy of Home Credit Default Risk EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BfnesP_V7aUxf77byfqaYTbG05y35NGO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!pip -q install duckdb
import duckdb
con = duckdb.connect()

from google.colab import drive
drive.mount('/content/drive')

"""# Important Glossary



*   Home Credit - The institution thats providing the loan based on customers who do not have much of a credit score detail in their system, and the institution for whom we are building the model

*   Credit Bureau -   
    1.   Third party
    2.   Loans that client get from other instituions get reported to Credit Bureau
    3.   Home Credit can extract data from Credit Bureau

# **Application_train.csv**

*   Data from **Home Credit**
*   application_train.csv has the target labels
*   One row represents one loan
*   Has other static data of customers
*   SK_ID_CURR is the customer unique indentifier
"""

application_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/application_train.csv')

application_df.head()

application_df.TARGET.value_counts()

application_df.shape

# adding number of missing values per row

application_df["TOT_MISSING_COUNT"] = application_df.isna().sum(axis=1)

application_df

EPS = 1e-6



application_df_preprocess = con.execute(f"""
SELECT

    ----------------- as-is features ---------------

    SK_ID_CURR,
    TARGET,

    EXT_SOURCE_1,
    EXT_SOURCE_2,
    EXT_SOURCE_3,

    AMT_ANNUITY,
    AMT_CREDIT,
    AMT_GOODS_PRICE,
    AMT_INCOME_TOTAL,

    DAYS_BIRTH,
    DAYS_EMPLOYED,
    DAYS_REGISTRATION,
    DAYS_ID_PUBLISH,
    DAYS_LAST_PHONE_CHANGE,

    CODE_GENDER,

    CNT_CHILDREN,
    CNT_FAM_MEMBERS,

    REGION_RATING_CLIENT,
    REGION_RATING_CLIENT_W_CITY,
    REGION_POPULATION_RELATIVE,

    WEEKDAY_APPR_PROCESS_START,
    HOUR_APPR_PROCESS_START,

    NAME_INCOME_TYPE,
    NAME_EDUCATION_TYPE,
    NAME_HOUSING_TYPE,
    NAME_CONTRACT_TYPE,
    NAME_FAMILY_STATUS,



    REG_REGION_NOT_LIVE_REGION,
    REG_REGION_NOT_WORK_REGION,
    LIVE_REGION_NOT_WORK_REGION,
    REG_CITY_NOT_LIVE_CITY,
    REG_CITY_NOT_WORK_CITY,
    LIVE_CITY_NOT_WORK_CITY,

    OBS_30_CNT_SOCIAL_CIRCLE,
    OBS_60_CNT_SOCIAL_CIRCLE,
    DEF_30_CNT_SOCIAL_CIRCLE,
    DEF_60_CNT_SOCIAL_CIRCLE,

    AMT_REQ_CREDIT_BUREAU_HOUR,
    AMT_REQ_CREDIT_BUREAU_DAY,
    AMT_REQ_CREDIT_BUREAU_WEEK,
    AMT_REQ_CREDIT_BUREAU_MON,
    AMT_REQ_CREDIT_BUREAU_QRT,
    AMT_REQ_CREDIT_BUREAU_YEAR,

    FLAG_MOBIL,
    FLAG_EMP_PHONE,
    FLAG_WORK_PHONE,
    FLAG_CONT_MOBILE,
    FLAG_PHONE,
    FLAG_EMAIL,

    FLAG_OWN_CAR,
    FLAG_OWN_REALTY,

    FLAG_DOCUMENT_2,
    FLAG_DOCUMENT_3,
    FLAG_DOCUMENT_4,
    FLAG_DOCUMENT_5,
    FLAG_DOCUMENT_6,
    FLAG_DOCUMENT_7,
    FLAG_DOCUMENT_8,
    FLAG_DOCUMENT_9,
    FLAG_DOCUMENT_10,
    FLAG_DOCUMENT_11,
    FLAG_DOCUMENT_12,
    FLAG_DOCUMENT_13,
    FLAG_DOCUMENT_14,
    FLAG_DOCUMENT_15,
    FLAG_DOCUMENT_16,
    FLAG_DOCUMENT_17,
    FLAG_DOCUMENT_18,
    FLAG_DOCUMENT_19,
    FLAG_DOCUMENT_20,
    FLAG_DOCUMENT_21,

    TOT_MISSING_COUNT,

    ----------------- modified time features ---------------

    -DAYS_BIRTH/(365 + {EPS}) AS AGE_YEARS,
    CASE WHEN DAYS_EMPLOYED = 365243 THEN 1 ELSE 0 END AS DAYS_EMPLOYED_ANOMALY,
    -CASE WHEN DAYS_EMPLOYED = 365243 THEN NULL ELSE DAYS_EMPLOYED END / (365 + {EPS}) AS EMPLOYED_YEARS,
    -CASE WHEN DAYS_EMPLOYED = 365243 THEN NULL ELSE DAYS_EMPLOYED END / (-DAYS_BIRTH + {EPS}) AS EMPLOYMENT_YEARS_TO_AGE,

    ----------------- ratio features ---------------

    AMT_ANNUITY/(AMT_CREDIT + {EPS}) AS PAYMENT_RATE,
    AMT_CREDIT/(AMT_INCOME_TOTAL + {EPS}) AS CREDIT_TO_INCOME,
    AMT_ANNUITY/(AMT_INCOME_TOTAL + {EPS}) AS ANNUITY_TO_INCOME,
    AMT_GOODS_PRICE/(AMT_CREDIT + {EPS}) AS GOODS_TO_CREDIT,
    AMT_INCOME_TOTAL/(CNT_FAM_MEMBERS + {EPS}) AS INCOME_PER_PERSON,
    CNT_CHILDREN/(CNT_FAM_MEMBERS + {EPS}) AS CHILDREN_RATIO,

    ----------------- missing-count features ---------------

    CASE WHEN EXT_SOURCE_1 IS NULL THEN 1 ELSE 0 END AS EXT_SOURCE_1_IS_MISSING,
    CASE WHEN EXT_SOURCE_2 IS NULL THEN 1 ELSE 0 END AS EXT_SOURCE_2_IS_MISSING,
    CASE WHEN EXT_SOURCE_3 IS NULL THEN 1 ELSE 0 END AS EXT_SOURCE_3_IS_MISSING,
    CASE WHEN OWN_CAR_AGE IS NULL THEN 1 ELSE 0 END AS  OWN_CAR_AGE_IS_MISSING,

    ----------------- count features ---------------

    COALESCE(FLAG_DOCUMENT_2,0) + COALESCE(FLAG_DOCUMENT_3,0) + COALESCE(FLAG_DOCUMENT_4,0) + COALESCE(FLAG_DOCUMENT_5,0) +
    COALESCE(FLAG_DOCUMENT_6,0) + COALESCE(FLAG_DOCUMENT_7,0) + COALESCE(FLAG_DOCUMENT_8,0) + COALESCE(FLAG_DOCUMENT_9,0) +
    COALESCE(FLAG_DOCUMENT_10,0) + COALESCE(FLAG_DOCUMENT_11,0) + COALESCE(FLAG_DOCUMENT_12,0) + COALESCE(FLAG_DOCUMENT_13,0) +
    COALESCE(FLAG_DOCUMENT_14,0) + COALESCE(FLAG_DOCUMENT_15,0) + COALESCE(FLAG_DOCUMENT_16,0) + COALESCE(FLAG_DOCUMENT_17,0) +
    COALESCE(FLAG_DOCUMENT_18,0) + COALESCE(FLAG_DOCUMENT_19,0) + COALESCE(FLAG_DOCUMENT_20,0) + COALESCE(FLAG_DOCUMENT_21,0)  AS DOC_COUNT,

    COALESCE(FLAG_MOBIL,0) + COALESCE(FLAG_EMP_PHONE,0) + COALESCE(FLAG_WORK_PHONE,0) +
    COALESCE(FLAG_CONT_MOBILE,0) + COALESCE(FLAG_PHONE,0) + COALESCE(FLAG_EMAIL,0) AS CONTACT_COUNT,

    COALESCE(REG_REGION_NOT_LIVE_REGION,0) + COALESCE(REG_REGION_NOT_WORK_REGION,0) + COALESCE(LIVE_REGION_NOT_WORK_REGION,0) +
    COALESCE(REG_CITY_NOT_LIVE_CITY,0) + COALESCE(REG_CITY_NOT_WORK_CITY,0) + COALESCE(LIVE_CITY_NOT_WORK_CITY,0) AS ADDR_MISMATCH_SUM

FROM
      application_df
""").df()

application_df_preprocess







"""# Bureau DF"""

bureau_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/bureau.csv')

bureau_df.head()

bureau_df_preprocess = con.execute(f"""
SELECT
      SK_ID_CURR,
      SUM(CASE WHEN CREDIT_ACTIVE = 'Active' THEN 1 ELSE 0 END) AS BUREAU_N_ACTIVE,
      SUM(CASE WHEN CREDIT_ACTIVE = 'Closed' THEN 1 ELSE 0 END) AS BUREAU_N_CLOSED,
      MAX(DAYS_CREDIT) AS BUREAU_LAST_CREDIT_DAYS,
      MAX(DAYS_CREDIT_UPDATE) AS BUREAU_LAST_UPDATE_DAYS,
      SUM(AMT_CREDIT_SUM_DEBT) AS BUREAU_TOTAL_DEBT,
      SUM(AMT_CREDIT_SUM) AS BUREAU_TOTAL_CREDIT,
      SUM(AMT_CREDIT_SUM_LIMIT) AS BUREAU_LIMIT_SUM,
      AVG(AMT_CREDIT_SUM_DEBT/(AMT_CREDIT_SUM + {EPS})) AS BUREAU_UTIL_MEAN,
      SUM(AMT_CREDIT_SUM_OVERDUE) AS BUREAU_OVERDUE_SUM,
      MAX(AMT_CREDIT_MAX_OVERDUE) AS BUREAU_MAX_OVERDUE,
      SUM(CNT_CREDIT_PROLONG) AS BUREAU_PROLONG_SUM
FROM
      bureau_df
GROUP BY
      SK_ID_CURR
""").df()

"""# Bureau Balance"""

bureau_balance_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/bureau_balance.csv')

bureau_balance_df_preprocess = con.execute("""
WITH table_1 AS (
    SELECT
          *,
          CASE WHEN (STATUS = 'C' OR STATUS = 'X' OR STATUS = '0') THEN 0 ELSE CAST(STATUS AS INT) END AS STATUS_NUM
    FROM
          bureau_balance_df
),

table_2 AS (
    SELECT
        *,
        CASE WHEN STATUS_NUM > 0 THEN 1 ELSE 0 END AS IS_DELINQ
    FROM
        table_1

),

table_3 AS (
    SELECT
        SK_ID_BUREAU,
        AVG(IS_DELINQ) AS BB_DELINQ_SHARE,
        MAX(STATUS_NUM) AS BB_WORST_STATUS_NUM
    FROM
        table_2
    GROUP BY
        SK_ID_BUREAU

),

table_4 AS (
    SELECT
          SK_ID_BUREAU,
          -MAX(MONTHS_BALANCE) AS BB_MONTHS_LAST_DELINQ
    FROM
          table_2
    WHERE
          IS_DELINQ = 1
    GROUP BY
          SK_ID_BUREAU

)

SELECT
      A.*,
      B.BB_MONTHS_LAST_DELINQ
FROM
      table_3 A
LEFT JOIN
      table_4 B
ON
      A.SK_ID_BUREAU = B.SK_ID_BUREAU
""").df()

bureau_balance_join_df = con.execute("""
WITH table_1 AS (
    SELECT
          A.*,
          B.SK_ID_CURR
    FROM
          bureau_balance_df_preprocess A
    LEFT JOIN
          bureau_df B
    ON
          A.SK_ID_BUREAU = B.SK_ID_BUREAU
),

table_2 AS (
    SELECT
        SK_ID_CURR,
        AVG(BB_DELINQ_SHARE) AS BB_DELINQ_SHARE_MEAN,
        MAX(BB_WORST_STATUS_NUM) AS BB_WORST_STATUS_MAX,
        MIN(BB_MONTHS_LAST_DELINQ) AS BB_MONTHS_SINCE_LAST_DELINQ_MIN
    FROM
        table_1
    GROUP BY
        SK_ID_CURR
)

SELECT * FROM table_2
""").df()

bureau_df_preproces_2 = con.execute("""
SELECT
      A.*,
      B.BB_DELINQ_SHARE_MEAN,
      B.BB_WORST_STATUS_MAX,
      B.BB_MONTHS_SINCE_LAST_DELINQ_MIN
FROM
      bureau_df_preprocess A
LEFT JOIN
      bureau_balance_join_df B
ON
      A.SK_ID_CURR = B.SK_ID_CURR
""").df()

"""# Prev Application"""

prev_application_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/previous_application.csv')

prev_application_df_preprocess = con.execute("""
SELECT
      SK_ID_CURR,
      COUNT(DISTINCT SK_ID_PREV) AS PREV_TOTAL,

      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Approved' THEN 1 ELSE 0 END) AS PREV_APPROVED,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Refused' THEN 1 ELSE 0 END) AS PREV_REFUSED,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Canceled' THEN 1 ELSE 0 END) AS PREV_CANCELLED,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Unused offer' THEN 1 ELSE 0 END) AS PREV_UNUSED,

      AVG(AMT_APPLICATION/AMT_CREDIT) AS PREV_APP_CREDIT_RATIO_MEAN,
      AVG(AMT_DOWN_PAYMENT/AMT_CREDIT) AS PREV_DOWNPAYMENT_RATE_MEAN,

      AVG(CNT_PAYMENT) AS PREV_CNT_PAYMENT_MEAN,
      MAX(DAYS_DECISION) AS PREV_LAST_DECISION_DAYS,
      MIN(DAYS_DECISION) AS PREV_EARLIEST_DECISION_DAYS,

      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Approved' THEN 1 ELSE 0 END)/COUNT(DISTINCT SK_ID_PREV) AS PREV_APPROVAL_RATE,
      SUM(CASE WHEN NAME_CONTRACT_STATUS = 'Refused' THEN 1 ELSE 0 END)/COUNT(DISTINCT SK_ID_PREV) AS PREV_REFUSED_RATE

FROM
    prev_application_df
GROUP BY
    SK_ID_CURR
""").df()

"""# Installments Payment"""

installments_payment_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/installments_payments.csv')

installments_payment_df.columns

installments_payment_df_preprocess = con.execute(f"""
WITH table_1 AS (
    SELECT
          *,
          AMT_PAYMENT/(AMT_INSTALMENT + {EPS}) AS PAYMENT_RATIO,
          DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT AS LATE_DAYS
    FROM
          installments_payment_df
),

table_2 AS (
    SELECT
          SK_ID_PREV,
          AVG(PAYMENT_RATIO) AS INSTAL_PAYMENT_RATIO_MEAN,
          STDDEV(PAYMENT_RATIO) AS INSTAL_PAYMENT_RATIO_STD,
          AVG(LATE_DAYS) AS INSTAL_LATE_MEAN,
          MAX(LATE_DAYS) AS INSTAL_LATE_MAX,
          AVG(CASE WHEN LATE_DAYS>0 THEN 1 ELSE 0 END) AS INSTAL_LATE_SHARE,
          AVG(CASE WHEN LATE_DAYS>0 THEN 30 ELSE 0 END) AS INSTAL_SEVERE_LATE_SHARE
    FROM
          table_1
    GROUP BY
          SK_ID_PREV
)

SELECT * FROM table_2
""").df()

installments_payment_join_df = con.execute("""
WITH table_1 AS (
    SELECT
          A.*,
          B.SK_ID_CURR
    FROM
          installments_payment_df_preprocess A
    LEFT JOIN
          prev_application_df B
          ON A.SK_ID_PREV = B.SK_ID_PREV
),


table_2 AS (
    SELECT
          SK_ID_CURR,
          AVG(INSTAL_PAYMENT_RATIO_MEAN) AS INSTAL_PAYMENT_RATIO_MEAN,
          AVG(INSTAL_PAYMENT_RATIO_STD) AS INSTAL_PAYMENT_RATIO_STD,
          AVG(INSTAL_LATE_MEAN) AS INSTAL_LATE_MEAN,
          MAX(INSTAL_LATE_MAX) AS INSTAL_LATE_MAX,
          AVG(INSTAL_LATE_SHARE) AS INSTAL_LATE_SHARE,
          AVG(INSTAL_SEVERE_LATE_SHARE) AS INSTAL_SEVERE_LATE_SHARE
    FROM
          table_1
    GROUP BY
          SK_ID_CURR

)

SELECT * FROM table_2
""").df()

"""# Credit Card Balance"""

creditcard_balance_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/credit_card_balance.csv')

creditcard_balance_df_preprocess = con.execute(f"""
WITH table_1 AS (
    SELECT
          *,
          AMT_BALANCE / (AMT_CREDIT_LIMIT_ACTUAL + {EPS}) AS CC_UTIL_ROW,
          AMT_PAYMENT_TOTAL_CURRENT / (AMT_INST_MIN_REGULARITY + {EPS}) AS CC_MINPAY_COVERAGE_ROW,
          CASE WHEN (SK_DPD > 0) OR (SK_DPD_DEF > 0) THEN 1 ELSE 0 END AS CC_DPD_ANY_ROW,
          AMT_DRAWINGS_ATM_CURRENT / (AMT_CREDIT_LIMIT_ACTUAL + {EPS}) AS CC_CASH_RATIO_ROW
    FROM
          creditcard_balance_df
),

table_2 AS (
    SELECT
          SK_ID_PREV,
          AVG(CC_UTIL_ROW) AS CC_UTIL_MEAN,
          MAX(CC_UTIL_ROW) AS CC_UTIL_MAX,
          AVG(CC_MINPAY_COVERAGE_ROW) AS CC_MINPAY_COVERAGE_MEAN,
          AVG(CASE WHEN CC_MINPAY_COVERAGE_ROW >=1 THEN 1 ELSE 0 END) AS CC_MINPAY_MET_SHARE,
          AVG(CC_DPD_ANY_ROW) AS CC_DPD_ANY_SHARE,
          MAX(SK_DPD) AS CC_DPD_MAX,
          AVG(CC_CASH_RATIO_ROW) AS CC_CASH_RATIO_MEAN
    FROM
          table_1
    GROUP BY
          SK_ID_PREV


)

SELECT * FROM table_2
""").df()

creditcard_balance_join_df = con.execute("""
WITH table_1 AS (
    SELECT
          A.*,
          B.SK_ID_CURR
    FROM
          creditcard_balance_df_preprocess A
    LEFT JOIN
          prev_application_df B
          ON A.SK_ID_PREV = B.SK_ID_PREV
),


table_2 AS (
    SELECT
          SK_ID_CURR,
          AVG(CC_UTIL_MEAN) AS CC_UTIL_MEAN,
          MAX(CC_UTIL_MAX) AS CC_UTIL_MAX,
          AVG(CC_MINPAY_COVERAGE_MEAN) AS CC_MINPAY_COVERAGE_MEAN,
          AVG(CC_MINPAY_MET_SHARE) AS CC_MINPAY_MET_SHARE,
          AVG(CC_DPD_ANY_SHARE) AS CC_DPD_ANY_SHARE,
          MAX(CC_DPD_MAX) AS CC_DPD_MAX,
          AVG(CC_CASH_RATIO_MEAN) AS CC_CASH_RATIO_MEAN
    FROM
          table_1
    GROUP BY
          SK_ID_CURR

)

SELECT * FROM table_2
""").df()

"""# POS_CASH_balance.csv"""

poscash_balance_df = pd.read_csv('/content/drive/MyDrive/home_credit_default_data/POS_CASH_balance.csv')

poscash_balance_df_preprocess = con.execute(f"""
WITH table_1 AS (
    SELECT
          *,
          CASE WHEN (SK_DPD > 0) or (SK_DPD_DEF > 0) THEN 1 ELSE 0 END AS POS_DPD_ANY_ROW
    FROM
          poscash_balance_df
),


table_2 AS (
    SELECT
          SK_ID_PREV,
          AVG(POS_DPD_ANY_ROW) AS POS_DPD_ANY_SHARE,
          MAX(SK_DPD) AS POS_DPD_MAX,
          -MAX(CASE WHEN NAME_CONTRACT_STATUS = 'Active' THEN MONTHS_BALANCE ELSE NULL END) AS POS_MONTHS_SINCE_LAST_ACTIVE

    FROM
          table_1
    GROUP BY
          SK_ID_PREV

)

SELECT * FROM table_2
""").df()

poscash_balance_join_df = con.execute("""
WITH table_1 AS (
    SELECT
          A.*,
          B.SK_ID_CURR
    FROM
          poscash_balance_df_preprocess A
    LEFT JOIN
          prev_application_df B
          ON A.SK_ID_PREV = B.SK_ID_PREV
),


table_2 AS (
    SELECT
          SK_ID_CURR,
          AVG(POS_DPD_ANY_SHARE) AS POS_DPD_ANY_SHARE,
          MAX(POS_DPD_MAX) AS POS_DPD_MAX,
          MAX(POS_MONTHS_SINCE_LAST_ACTIVE) AS POS_MONTHS_SINCE_LAST_ACTIVE
    FROM
          table_1
    GROUP BY
          SK_ID_CURR

)

SELECT * FROM table_2
""").df()







[col for col in poscash_balance_join_df.columns]

"""#MODEL BUILDING 1"""

import pandas as pd

b_cols = [
    "SK_ID_CURR",
    "BUREAU_N_ACTIVE","BUREAU_N_CLOSED","BUREAU_LAST_CREDIT_DAYS","BUREAU_LAST_UPDATE_DAYS",
    "BUREAU_TOTAL_DEBT","BUREAU_TOTAL_CREDIT","BUREAU_LIMIT_SUM","BUREAU_UTIL_MEAN",
    "BUREAU_OVERDUE_SUM","BUREAU_MAX_OVERDUE","BUREAU_PROLONG_SUM",
    "BB_DELINQ_SHARE_MEAN","BB_WORST_STATUS_MAX","BB_MONTHS_SINCE_LAST_DELINQ_MIN"
]

c_cols = [
    "SK_ID_CURR",
    "PREV_TOTAL","PREV_APPROVED","PREV_REFUSED","PREV_CANCELLED","PREV_UNUSED",
    "PREV_APP_CREDIT_RATIO_MEAN","PREV_DOWNPAYMENT_RATE_MEAN","PREV_CNT_PAYMENT_MEAN",
    "PREV_LAST_DECISION_DAYS","PREV_EARLIEST_DECISION_DAYS",
    "PREV_APPROVAL_RATE","PREV_REFUSED_RATE"
]

d_cols = ['SK_ID_CURR',
 'INSTAL_PAYMENT_RATIO_MEAN',
 'INSTAL_PAYMENT_RATIO_STD',
 'INSTAL_LATE_MEAN',
 'INSTAL_LATE_MAX',
 'INSTAL_LATE_SHARE',
 'INSTAL_SEVERE_LATE_SHARE']

e_cols = ['SK_ID_CURR',
 'CC_UTIL_MEAN',
 'CC_UTIL_MAX',
 'CC_MINPAY_COVERAGE_MEAN',
 'CC_MINPAY_MET_SHARE',
 'CC_DPD_ANY_SHARE',
 'CC_DPD_MAX',
 'CC_CASH_RATIO_MEAN']

f_cols = ['SK_ID_CURR',
 'POS_DPD_ANY_SHARE',
 'POS_DPD_MAX',
 'POS_MONTHS_SINCE_LAST_ACTIVE']

feature_store_df = (
    application_df_preprocess
      .merge(bureau_df_preproces_2[b_cols], on="SK_ID_CURR", how="left", validate="one_to_one")
      .merge(prev_application_df_preprocess[c_cols], on="SK_ID_CURR", how="left", validate="one_to_one")
      .merge(installments_payment_join_df[d_cols], on="SK_ID_CURR", how="left", validate="one_to_one")
      .merge(creditcard_balance_join_df[e_cols], on="SK_ID_CURR", how="left", validate="one_to_one")
      .merge(poscash_balance_join_df[f_cols], on="SK_ID_CURR", how="left", validate="one_to_one")
)

# feature_store_df = application_df_preprocess.copy()

categorical_columns = feature_store_df.select_dtypes(include=['object']).columns
categorical_columns

# one-hot encoding categorical columns

feature_store_df = pd.get_dummies(feature_store_df, columns=categorical_columns)
[col for col in feature_store_df.columns]

# impute missing values with median

feature_store_df = feature_store_df.fillna(feature_store_df.median(numeric_only=True))

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(feature_store_df, test_size= 0.2, stratify=feature_store_df.TARGET, random_state=42)

print(train_df.TARGET.value_counts())
print(test_df.TARGET.value_counts())

# Making balanced training set

target_1 = train_df[train_df.TARGET == 1]
target_0 = train_df[train_df.TARGET == 0].sample(n = target_1.shape[0])

print(target_0.shape[0], target_1.shape[0])

final_train_df = pd.concat([target_0, target_1], axis = 0)
print(final_train_df.TARGET.value_counts())

X_cols = [col for col in final_train_df.columns]
X_cols = [c for c in X_cols if c not in {"TARGET", "SK_ID_CURR"}]

y_col = 'TARGET'

from xgboost import XGBClassifier

model = XGBClassifier()

model.fit(final_train_df[X_cols], final_train_df[y_col])

y_pred = model.predict(test_df[X_cols])
y_test = test_df[y_col]

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix
)
import numpy as np



acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec  = recall_score(y_test, y_pred, zero_division=0)
f1   = f1_score(y_test, y_pred, zero_division=0)

print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}")

#  summary per class
print("\nClassification report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion matrix:\n", cm)

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score
)
import numpy as np

# Ensure y_test is a 1D Series even if y_col is a list
y_name = y_col[0] if isinstance(y_col, (list, tuple)) else y_col
y_test = test_df[y_name]

# Predictions (labels)
y_pred = model.predict(test_df[X_cols])  # <- fixed from xy_pred

# --- Metrics (thresholded labels) ---
acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec  = recall_score(y_test, y_pred, zero_division=0)
f1   = f1_score(y_test, y_pred, zero_division=0)

print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}")

print("\nClassification report:")
print(classification_report(y_test, y_pred, zero_division=0))

cm = confusion_matrix(y_test, y_pred)
print("\nConfusion matrix:\n", cm)

# --- ROC–AUC (uses scores/probabilities, not labels) ---
# Try to get per-class scores
y_score = None
score_matrix = None  # keep full matrix for multiclass

if hasattr(model, "predict_proba"):
    score_matrix = model.predict_proba(test_df[X_cols])
elif hasattr(model, "decision_function"):
    score_matrix = model.decision_function(test_df[X_cols])

if score_matrix is not None:
    # If 1D, make it 2D for uniform handling
    if np.ndim(score_matrix) == 1:
        score_matrix = np.asarray(score_matrix).reshape(-1, 1)

    # Determine number of classes from scores or y_test
    n_classes = score_matrix.shape[1] if score_matrix.ndim == 2 else 1
    if n_classes <= 1:
        # Binary case but only one column returned (rare) — fall back to y_pred
        y_score = np.asarray(score_matrix).ravel() if score_matrix is not None else None
    else:
        # If binary, use positive-class column for standard ROC–AUC
        if n_classes == 2:
            y_score = score_matrix[:, 1]
        else:
            # Multiclass: keep full matrix
            y_score = score_matrix

# Print AUC
if y_score is None:
    print("\nROC–AUC: unavailable (model exposes neither predict_proba nor decision_function).")
else:
    try:
        if y_score.ndim == 1 or y_score.shape[1] == 2:
            # Binary ROC–AUC
            # If we accidentally passed two columns, use the positive one
            if y_score.ndim == 2 and y_score.shape[1] == 2:
                y_score = y_score[:, 1]
            auc = roc_auc_score(y_test, y_score)
            print(f"\nROC–AUC (binary): {auc:.4f}")
        else:
            # Multiclass ROC–AUC (report both OVR and OVO)
            auc_ovr = roc_auc_score(y_test, y_score, multi_class="ovr")
            auc_ovo = roc_auc_score(y_test, y_score, multi_class="ovo")
            print(f"\nROC–AUC (multiclass, OVR): {auc_ovr:.4f}")
            print(f"ROC–AUC (multiclass, OVO): {auc_ovo:.4f}")
    except ValueError as e:
        # Handles cases like single-class y_test in the slice
        print(f"\nROC–AUC: could not be computed ({e}).")

